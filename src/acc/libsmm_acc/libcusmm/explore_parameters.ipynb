{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# *libcusmm*: explore the parameter space "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "* smem usage as computed in Python script isn't correct: \n",
    "\t* run ``nvcc -O3 -arch=sm_60 -Xptxas -v -w -c tune_25x32x5_exe0_part0.cu`` to find out \n",
    "\t* It seems to be correct for tiny, but not for the other algorithms \n",
    "\t* I cannot seem to find a pattern or understand the non-constant offset\n",
    "* number of registers used should be predictor as well (obtain by above) \n",
    "* often, the two best performing configs have ``tile_mA * tile_nA = tile_mB * tile_nB``. Perhaps ``tile_m * tile_n` should be a predictor? \n",
    "\n",
    "#### Bounds and constraints\n",
    "\n",
    "* Threads (min, max) \n",
    "\t* rare cases in which going above brings ~4% perf improvement\n",
    "\n",
    "* Max parallel work \n",
    "\n",
    "* tile_m, tile_n (min, max) \n",
    "\t* values above: within noise level and in cases where bigger T schadet nicht\n",
    "\n",
    "#### Stack size \n",
    "\n",
    "* used in autotuning: 16'005\n",
    "* used in DBCSR with smm_acc: 30'000 (./core/dbcsr_config.F, mm_stack_default_size)\n",
    "* A. B get chopped up and a list of stacks gets filled. Whenever it is full -> flush to multiply \n",
    "\t* (there is a shorter list of remainder at the end) \n",
    "\n",
    "#### Todo \n",
    "\n",
    "* find constraints on w, v\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from tune_13x28x32/data_dump\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import explore_parameters_utils as explore_utils\n",
    "\n",
    "# Helper variables and functions\n",
    "algo_dict = {'tiny': 0, 'small': 1, 'medium': 2, 'largeDB1': 3, 'largeDB2': 4}  # category-encoder \n",
    "algo_dict_rev = {0: 'tiny', 1: 'small', 2: 'medium', 3: 'largeDB1', 4: 'largeDB2'}\n",
    "npars = 3   # number of parameters in stack list\n",
    "def format_pars(df):\n",
    "    df.replace(to_replace=algo_dict, inplace=True)\n",
    "    df.fillna(value=0, inplace=True)\n",
    "    df = df.rename(columns={'threads': 'threads_per_blk', 'nregs': 'regs_per_thread'})\n",
    "    return df \n",
    "\n",
    "# Read GPU properties\n",
    "gpu = pd.read_excel('parameters_P100.xlsx', sheet_name='GPU', header=0)\n",
    "gpu = gpu.to_dict(orient='list')\n",
    "for k, v in gpu.items():\n",
    "    gpu[k] = v[0]\n",
    "\n",
    "# Read autotuning settings\n",
    "autotuning = pd.read_excel('parameters_P100.xlsx', sheet_name='Autotuning', header=0)\n",
    "autotuning = autotuning.to_dict()\n",
    "for k, v in autotuning.items():\n",
    "    autotuning[k] = v[0]\n",
    "\n",
    "# Read autotuning data\n",
    "pars_autotuned = pd.read_excel('parameters_P100.xlsx', sheet_name='parameters_P100', header=0)\n",
    "pars_autotuned = format_pars(pars_autotuned);\n",
    "##mnk_kernel = 'tune_4x4x4_test-stacksize_1605'\n",
    "##mnk_kernel = 'tune_4x4x5_test-stacksize_1605'\n",
    "##mnk_kernel = 'tune_4x4x8_test-stacksize_1605'\n",
    "##mnk_kernel = 'tune_4x4x32_test-stacksize_1605'\n",
    "##mnk_kernel = 'tune_26x4x13'\n",
    "mnk_kernel = 'tune_13x28x32'\n",
    "##mnk_kernel = 'tune_13x28x45'\n",
    "pars_autotuning = explore_utils.read_kernel_autotuning(mnk_kernel)\n",
    "pars_autotuning = format_pars(pars_autotuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Physical Limits for GPU Compute Capability:': 6, 'Threads per Warp': 32, 'Max Warps per Multiprocessor': 64, 'Max Thread Blocks per Multiprocessor': 32, 'Max Threads per Multiprocessor': 2048, 'Maximum Thread Block Size': 1024, 'Registers per Multiprocessor': 65536, 'Max Registers per Thread Block': 65536, 'Max Registers per Thread': 255, 'Shared Memory per Multiprocessor (bytes)': 65536, 'Max Shared Memory per Block': 49152, 'Register allocation unit size': 256, 'Register allocation granularity': 'warp', 'Shared Memory allocation unit size': 256, 'Warp allocation granularity': 2, 'Multiprocessors': 56}\n"
     ]
    }
   ],
   "source": [
    "print(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_a': 10000, 'n_b': 10000, 'n_c': 1000, 'stack_size': 16005, 'sizeof int': 4, 'sizeof double': 8}\n"
     ]
    }
   ],
   "source": [
    "print(autotuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictors and features\n",
    "\n",
    "## Combine predictors and incorporate GPU properties "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {margin-left: 0 !important;}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {margin-left: 0 !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Input parameters \"raw\"\n",
    "\n",
    "| kernel parameters | (raw kernel specifications) |\n",
    "| :-----------------: | :---------------: |\n",
    "| m, n, k           |       |\n",
    "| algorithm         | 0:tiny, 1:small, 2:medium, 3:largeDB1, 4:largeDB2 |\n",
    "| tile_m, tile_n    |       |\n",
    "| w, v              |       |\n",
    "| threads_per_blk   |       |\n",
    "| grouping          | loosely corresponds to nrun |\n",
    "| minblocks         |       |\n",
    "\n",
    "| compilation       | (obtained from compilation log) |\n",
    "| ----------------- |---------------|\n",
    "| regs_per_thread   |               |\n",
    "| nbytes_smem       |               |\n",
    "| nbytes_cmem       |               |\n",
    "\n",
    "\n",
    "| autotuning        |  |\n",
    "| ----------------- |---------------|\n",
    "| stack size        |       |\n",
    "\n",
    "\n",
    "| GPU               | (obtained from CUDA) |\n",
    "| ----------------- |---------------|\n",
    "\n",
    "__________\n",
    "\n",
    "#### Input parameters \"derived\"\n",
    "\n",
    "| ?? | depends on |\n",
    "| :----------------- | :--------------- |\n",
    "| size_a, size_b, size_c       | m, n, k|\n",
    "| need_sync                    |        |  \n",
    "| ru_param_stack_unroll_factor |        |\n",
    "\n",
    "| Launch configuration | depends on |\n",
    "| ----------------- |---------------|\n",
    "| nblks             | stack_size, grouping |\n",
    "| warps_per_blk     |                      |\n",
    "| nwarps    |                      |\n",
    "| nthreads    |                      |\n",
    "| sm_desired    |                      |\n",
    "| smem_per_block    |                      |\n",
    "| nblocks_per_sm_lim_blks_warps    |                      |\n",
    "| nblocks_per_sm_lim_reg    |                      |\n",
    "| nblocks_per_sm_lim_smem    |                      |\n",
    "| nblocks_per_sm    |                      |\n",
    "| nwarps_per_sm    |                      |\n",
    "| occupancy    |                      |\n",
    "       \n",
    "| resource usage estimation (tiny) | depends on |\n",
    "| ----------------- |---------------|\n",
    "| ru_tiny_max_parallel_work | |\n",
    "| ru_tiny_min_threads |\n",
    "| ru_tiny_max_threads |\n",
    "| ru_tiny_buf_size         |       |\n",
    "| ru_tiny_smem_per_block             |  |\n",
    "| ru_tiny_max_parallel_work         |       |\n",
    "| ru_tiny_min_threads         |       |\n",
    "| ru_tiny_max_threads         |       |\n",
    "| ru_tiny_buf_size         |       |\n",
    "| ru_tiny_smem_per_block         |       |\n",
    "\n",
    "| resource usage estimation (small, medium) | depends on |\n",
    "| ----------------- |---------------|\n",
    "| ru_smallmed_max_parallel_work | |\n",
    "| ru_smallmed_tm_max | |\n",
    "| ru_smallmed_tn_max | |\n",
    "| ru_smallmed_unroll_factor_c | |\n",
    "| ru_smallmed_cmax | |\n",
    "| ru_smallmed_rmax | |\n",
    "| ru_smallmed_loop_matmul | |\n",
    "| ru_smallmed_max_parallel_work | |\n",
    "       \n",
    "| resource usage estimation (tiny, small, medium) | depends on |\n",
    "| ----------------- |---------------|\n",
    "| ru_tinysmallmed_unroll_factor_a |        |\n",
    "| ru_tinysmallmed_unroll_factor_b |        |\n",
    "\n",
    "\n",
    "| resource usage estimation (largeDB1, largeDB2) | depends on |\n",
    "| ----------------- |---------------|\n",
    "| ru_large_Pa | m * w |\n",
    "| ru_large_Pb | w * n |\n",
    "| ru_large_Pc  | | \n",
    "| number of P_a, P_b | | \n",
    "| ru_large_unroll_factor_a | | \n",
    "| ru_large_unroll_factor_b | | \n",
    "| ru_large_unroll_factor_c | | \n",
    "| ru_large_loop_matmul | | \n",
    "       \n",
    "| resource usage estimation (small, medium, largeDB1, largeDB2) | depends on |\n",
    "| ----------------- |---------------|\n",
    "| ru_smallmedlarge_T|        |\n",
    "\n",
    "__________\n",
    "\n",
    "#### Outcome-variable\n",
    "\n",
    "1. perf (Gflop/s)\n",
    "2. performance squared [Gflop/s]^2\n",
    "3. boxcox(perf) [?]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![parameters dependency graph](ml_params.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def ceiling(x, step):\n",
    "    return np.where(x % step == 0, x, x + step - x % step)\n",
    "\n",
    "def flooring(x, step):\n",
    "    return np.where(x % step == 0, x, x - x % step)\n",
    "\n",
    "def ceil_division(a, b):\n",
    "    return (a + b - 1) // b\n",
    "\n",
    "def add_matrix_sizes(df):\n",
    "    df['size_a'] = df['m'] * df['k']\n",
    "    df['size_b'] = df['k'] * df['n']\n",
    "    df['size_c'] = df['m'] * df['n']\n",
    "\n",
    "def add_launch_pars(df): \n",
    "    # Need sync? (relevant for tiny, small, medium) \n",
    "    # (mn > warp_size || mk > warp_size || kn > warp_size || threads > warp_size);\n",
    "    df['need_sync'] = (np.where(df['size_c'] > gpu['Threads per Warp'], True, False)\n",
    "                       | np.where(df['size_a'] > gpu['Threads per Warp'], True, False) \n",
    "                       | np.where(df['size_b'] > gpu['Threads per Warp'], True, False)\n",
    "                       | np.where(df['threads_per_blk'] > gpu['Threads per Warp'], True, False)).tolist()\n",
    "    \n",
    "    # Launching parameters\n",
    "    df['nblks'] = ceil_division(autotuning['stack_size'], df['grouping'])\n",
    "    df['warps_per_blk'] = np.floor(df['threads_per_blk'] / gpu['Threads per Warp'])\n",
    "    df['nwarps'] = df['warps_per_blk'] * df['nblks']\n",
    "    df['nthreads'] = df['threads_per_blk'] * df['nblks']\n",
    "\n",
    "def add_occupancy_estimation(df, one_col): \n",
    "    df['sm_desired'] = ceil_division(df['nblks'], df['minblocks'])\n",
    "    \n",
    "    # Resource occupations (warps, blocks), (Follows CUDA calculator sheet)\n",
    "    df['nblocks_per_sm_lim_blks_warps'] = np.minimum(one_col * gpu['Max Thread Blocks per Multiprocessor'], \\\n",
    "                                                     np.floor(gpu['Max Warps per Multiprocessor'] / df['warps_per_blk']))\n",
    "\n",
    "    # Resource occupations (registers)\n",
    "    if 'regs_per_thread' in df.columns:\n",
    "        df['i1'] = flooring(gpu['Max Registers per Thread Block'] * one_col / ceiling(df['regs_per_thread'] * gpu['Threads per Warp'], gpu['Register allocation unit size'] * one_col), \n",
    "                            gpu['Warp allocation granularity'] * one_col)\n",
    "        df['nblocks_per_sm_lim_reg'] = np.floor(df['i1'] / df['warps_per_blk']) * \\\n",
    "                                       math.floor(gpu['Registers per Multiprocessor'] / gpu['Max Registers per Thread Block'])\n",
    "        df = df.drop('i1', axis=1)\n",
    "        \n",
    "    # Resource occupations (shared memory)\n",
    "    if 'nbytes_smem' in df.columns:\n",
    "        df['smem_per_block'] = ceiling(df['nbytes_smem'], gpu['Shared Memory allocation unit size'])\n",
    "        df['nblocks_per_sm_lim_smem'] = np.floor(one_col * gpu['Shared Memory per Multiprocessor (bytes)'] / df['smem_per_block'], one_col)\n",
    "        \n",
    "    # Aggregate \n",
    "    if 'nblocks_per_sm_lim_blks_warps' in df.columns and 'nblocks_per_sm_lim_reg' in df.columns and 'nblocks_per_sm_lim_smem' in df.columns :\n",
    "        df['nblks_per_sm'] = df[['nblocks_per_sm_lim_blks_warps', 'nblocks_per_sm_lim_reg', 'nblocks_per_sm_lim_smem']].min(axis=1)\n",
    "        df['nwarps_per_sm'] = df['nblks_per_sm'] * df['warps_per_blk']\n",
    "        df['nsm'] = ceiling(df['nblks'], df['nblks_per_sm'])\n",
    "        df['ngpu'] = ceiling(df['nsm'], gpu['Multiprocessors'])\n",
    "        df['occupancy'] = df['nwarps_per_sm'] / gpu['Max Warps per Multiprocessor']\n",
    "        \n",
    "def add_ru_common(df): \n",
    "    # Loop counts\n",
    "    df['ru_param_stack_unroll_factor'] = df['grouping'] // df['threads_per_blk']\n",
    "\n",
    "def add_ru_tiny(df):\n",
    "    # Resource usage estimation and loop counts (tiny) \n",
    "    df['ru_tinysmallmed_unroll_factor_a'] = df['size_a'] // df['threads_per_blk']\n",
    "    df['ru_tinysmallmed_unroll_factor_b'] = df['size_b'] // df['threads_per_blk']\n",
    "    df['ru_tiny_max_parallel_work'] = df[['grouping', 'size_a', 'size_b', 'size_c']].max(axis=1)\n",
    "    df['ru_tiny_min_threads'] = df['size_b']\n",
    "    df['ru_tiny_max_threads'] = ceiling(df['ru_tiny_max_parallel_work'], gpu['Threads per Warp'])\n",
    "    df['ru_tiny_buf_size'] = df['k'] * (df['m'] + df['n'])\n",
    "    if 'grouping' in df.columns:\n",
    "        df['ru_tiny_smem_per_block'] = (df['ru_tiny_buf_size'] * autotuning['sizeof double']) + (npars * df['grouping'] * autotuning['sizeof int']) \n",
    "    \n",
    "def add_ru_smallmed(df):\n",
    "    # Resource usage estimation and loop counts (small, medium) \n",
    "    df['ru_smallmed_tm_max'] = df['m']\n",
    "    df['ru_smallmed_tn_max'] = df['n']\n",
    "    df['ru_smallmed_unroll_factor_c'] = df['size_c'] // df['threads_per_blk']\n",
    "        \n",
    "    if 'tile_m' in df.columns and 'tile_n' in df.columns:\n",
    "        df['ru_smallmed_cmax'] = ceil_division(df['n'], df['tile_n'])\n",
    "        df['ru_smallmed_rmax'] = ceil_division(df['m'], df['tile_m'])\n",
    "        df['ru_smallmedlarge_T'] = df['tile_m'] * df['tile_n']\n",
    "        df['ru_smallmed_loop_matmul'] = df['k'] * df['tile_m'] * df['tile_n']\n",
    "            \n",
    "        df['intermediate'] = df['ru_smallmed_cmax'] * df['ru_smallmed_rmax']\n",
    "        df['ru_smallmed_max_parallel_work'] = df[['grouping', 'size_a', 'size_b', 'size_c', 'intermediate']].max(axis=1)\n",
    "        df = df.drop('intermediate', axis=1)\n",
    "        \n",
    "        df['ru_smallmed_min_threads'] = df['ru_smallmed_cmax'] * df['ru_smallmed_rmax']\n",
    "        df['ru_smallmed_max_threads'] = ceiling(df['ru_smallmed_max_parallel_work'], gpu['Threads per Warp'])\n",
    "        \n",
    "        df['intermediate1'] = df['size_a'] + df['k'] * df['tile_n'] * df['ru_smallmed_cmax']\n",
    "        df['intermediate2'] = df['tile_m'] * df['ru_smallmed_rmax'] * df['k'] + 1\n",
    "        df['ru_smallmed_buf_size'] = df[['size_c', 'intermediate1', 'intermediate2']].max(axis=1)\n",
    "        df = df.drop('intermediate1', axis=1)\n",
    "        df = df.drop('intermediate2', axis=1)\n",
    "\n",
    "        df['ru_smallmed_smem_per_block'] = (df['ru_smallmed_buf_size'] * autotuning['sizeof double']) + (npars * df['grouping'] * autotuning['sizeof int'])\n",
    "        \n",
    "def add_ru_large(df):\n",
    "    # Resource usage estimation and loop counts (largeDB1, largeDB2) \n",
    "    df['ru_large_Pa'] = df['m'] * df['w']  # Input slab sizes \n",
    "    df['ru_large_Pb'] = df['w'] * df['n']\n",
    "    df['ru_large_Pc'] = df['m'] * df['v']  # Output slabs\n",
    "    df['ru_large_unroll_factor_a'] = df['ru_large_Pa'] // df['threads_per_blk']\n",
    "    df['ru_large_unroll_factor_b'] = df['ru_large_Pb'] // df['threads_per_blk']\n",
    "    df['ru_large_unroll_factor_c'] = df['ru_large_Pc'] // df['threads_per_blk']\n",
    "    df['ru_large_loop_matmul'] = df['w'] * df['tile_m'] * df['tile_n']\n",
    "\n",
    "def add_derived_parameter(df):\n",
    "    one_col = np.ones(len(df['algo']))\n",
    "    add_matrix_sizes(df)\n",
    "    add_launch_pars(df)\n",
    "    add_occupancy_estimation(df, one_col)\n",
    "    add_ru_common(df)\n",
    "    add_ru_tiny(df)\n",
    "    add_ru_smallmed(df)\n",
    "    add_ru_large(df)\n",
    "\n",
    "# Call\n",
    "add_derived_parameter(pars_autotuned)\n",
    "add_derived_parameter(pars_autotuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformations\n",
    "\n",
    "look at: sklearn.preprocessing\n",
    "\n",
    "#### Feature scaling \n",
    "from sklearn.preprocessing import scale\n",
    "Xs = scale(X)\n",
    "\n",
    "#### Mean normalization\n",
    "\n",
    "#### Log-scale or other transf. \n",
    "\n",
    "Box-Cox, power transform in order to better capture large peformances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(X)\n",
    "rescaledX = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groups of predictors (for conveniance) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw predictors\n",
    "mnk = ['m', 'n', 'k']\n",
    "kernel_pars = ['algo', 'tile_m', 'tile_n', 'v', 'w', 'threads_per_blk', 'minblocks', 'grouping'] \n",
    "memory = ['nbytes_smem', 'regs_per_thread']\n",
    "predictors_raw = kernel_pars + memory\n",
    "\n",
    "# Auxiliaries \n",
    "sync = ['need_sync']\n",
    "ru = ['i1', ]\n",
    "ru_redundant = ['nthreads', 'occupancy', 'nblocks_per_sm', 'nblks']    # with: nwarps, nwarps_per_sm, nwarps_per_sm, nwarps\n",
    "ru_tiny = ['ru_tiny_max_parallel_work', 'ru_tiny_min_threads', 'ru_tiny_max_threads', 'ru_tiny_buf_size', 'ru_tiny_smem_per_block']\n",
    "ru_smallmed = ['ru_smallmed_tm_max', 'ru_smallmed_tn_max', 'ru_smallmed_cmax', 'ru_smallmed_rmax', 'ru_smallmed_max_parallel_work']\n",
    "nblks_lims = ['nblocks_per_sm_lim_blks_warps', 'nblocks_per_sm_lim_reg', 'nblocks_per_sm_lim_smem']\n",
    "cmem = ['nbytes_cmem']\n",
    "sm_desired = ['sm_desired']\n",
    "\n",
    "# Features \n",
    "matrix_sizes = ['size_a', 'size_b', 'size_c']\n",
    "kernel_pars_red = ['algo', 'tile_m', 'tile_n', 'v', 'w', 'minblocks', 'grouping']\n",
    "launch_pars = ['warps_per_blk', 'nwarps']\n",
    "occupancy = ['smem_per_block', 'nwarps_per_sm']\n",
    "predictor_features = kernel_pars_red + launch_pars + occupancy\n",
    "\n",
    "# Outcome\n",
    "perf = ['perf (Gflop/s)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data visualization / Exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Pars autotuned:\n",
      "Numer of column: 46 \n",
      "Number of rows: 1509\n",
      "Column names:\n",
      " Index(['algo', 'm', 'n', 'k', 'tile_m', 'tile_n', 'w', 'v', 'threads_per_blk',\n",
      "       'grouping', 'minblocks', 'perf (Gflop/s)', 'size_a', 'size_b', 'size_c',\n",
      "       'need_sync', 'nblks', 'warps_per_blk', 'nwarps', 'nthreads',\n",
      "       'sm_desired', 'nblocks_per_sm_lim_blks_warps',\n",
      "       'ru_param_stack_unroll_factor', 'ru_tinysmallmed_unroll_factor_a',\n",
      "       'ru_tinysmallmed_unroll_factor_b', 'ru_tiny_max_parallel_work',\n",
      "       'ru_tiny_min_threads', 'ru_tiny_max_threads', 'ru_tiny_buf_size',\n",
      "       'ru_tiny_smem_per_block', 'ru_smallmed_tm_max', 'ru_smallmed_tn_max',\n",
      "       'ru_smallmed_unroll_factor_c', 'ru_smallmed_cmax', 'ru_smallmed_rmax',\n",
      "       'ru_smallmedlarge_T', 'ru_smallmed_loop_matmul', 'intermediate',\n",
      "       'ru_smallmed_max_parallel_work', 'ru_large_Pa', 'ru_large_Pb',\n",
      "       'ru_large_Pc', 'ru_large_unroll_factor_a', 'ru_large_unroll_factor_b',\n",
      "       'ru_large_unroll_factor_c', 'ru_large_loop_matmul'],\n",
      "      dtype='object')\n",
      "\n",
      "--- Pars autotuning:\n",
      "Numer of column: 51 \n",
      "Number of rows: 54009\n",
      "Column names:\n",
      " Index(['algo', 'grouping', 'k', 'm', 'minblocks', 'n', 'nbytes_cmem',\n",
      "       'nbytes_smem', 'regs_per_thread', 'perf (Gflop/s)', 'threads_per_blk',\n",
      "       'tile_m', 'tile_n', 'v', 'w', 'size_a', 'size_b', 'size_c', 'need_sync',\n",
      "       'nblks', 'warps_per_blk', 'nwarps', 'nthreads', 'sm_desired',\n",
      "       'nblocks_per_sm_lim_blks_warps', 'i1', 'nblocks_per_sm_lim_reg',\n",
      "       'ru_param_stack_unroll_factor', 'ru_tinysmallmed_unroll_factor_a',\n",
      "       'ru_tinysmallmed_unroll_factor_b', 'ru_tiny_max_parallel_work',\n",
      "       'ru_tiny_min_threads', 'ru_tiny_max_threads', 'ru_tiny_buf_size',\n",
      "       'ru_tiny_smem_per_block', 'ru_smallmed_tm_max', 'ru_smallmed_tn_max',\n",
      "       'ru_smallmed_unroll_factor_c', 'ru_smallmed_cmax', 'ru_smallmed_rmax',\n",
      "       'ru_smallmedlarge_T', 'ru_smallmed_loop_matmul', 'intermediate',\n",
      "       'ru_smallmed_max_parallel_work', 'ru_large_Pa', 'ru_large_Pb',\n",
      "       'ru_large_Pc', 'ru_large_unroll_factor_a', 'ru_large_unroll_factor_b',\n",
      "       'ru_large_unroll_factor_c', 'ru_large_loop_matmul'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print('--- Pars autotuned:')\n",
    "print('Numer of column:', len(pars_autotuned.columns), '\\nNumber of rows:', len(pars_autotuned['algo']))\n",
    "print('Column names:\\n', pars_autotuned.columns)\n",
    "print('\\n--- Pars autotuning:')\n",
    "print('Numer of column:', len(pars_autotuning.columns), '\\nNumber of rows:', len(pars_autotuning['algo']))\n",
    "print('Column names:\\n', pars_autotuning.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Describe pars_autotuned\n",
    "for i, pa in enumerate(pars_autotuning): \n",
    "    print(\"\\n\\nParameters autotuning (\", i, \")\")\n",
    "    display(pa[mnk + matrix_sizes + perf].describe())\n",
    "    display(pa[kernel_pars].describe())\n",
    "    display(pa[memory].describe())\n",
    "    display(pa[launch_pars].describe())\n",
    "    display(pa[ru_tiny].describe())\n",
    "    display(pa[ru_smallmed].describe())\n",
    "    display(pa[occupancy].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Describe pars_autotuned\n",
    "display(pars_autotuned[mnk + matrix_sizes + perf].describe())\n",
    "display(pars_autotuned[kernel_pars].describe())\n",
    "display(pars_autotuned[launch_pars].describe())\n",
    "display(pars_autotuned[ru_tiny].describe())\n",
    "display(pars_autotuned[ru_smallmed].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Profiling\n",
    "\n",
    "#### Observations \n",
    "* There are A LOT of unnecessary (redundant) predictor variables, do not forget to sithe through \n",
    "* nbloks_per_sm is strongly positively correlated with performance (duh) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas_profiling \n",
    "pandas_profiling.ProfileReport(pars_autotuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pandas_profiling.ProfileReport(pars_autotuning[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pandas_profiling.ProfileReport(pars_autotuning[3][predictors_raw + perf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pandas_profiling.ProfileReport(pars_autotuning[3][predictor_features + perf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Histograms with Bokeh\n",
    "from bokeh.plotting import figure \n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "from bokeh.io import output_notebook, show\n",
    "output_notebook()\n",
    "\n",
    "num_bins = 200 \n",
    "toplot = pars_autotuning[3]['perf (Gflop/s)']\n",
    "\n",
    "# Create histogram\n",
    "hist, edges = np.histogram(toplot, bins=num_bins)\n",
    "df_hist = pd.DataFrame({'hist': hist, 'left': edges[:-1], 'right': edges[1:]})\n",
    "source = ColumnDataSource(df_hist)\n",
    "\n",
    "# Create tool \n",
    "hover = HoverTool(tooltips=[('# occurences', '@hist'), ('low', '@left'), ('high', '@right')])\n",
    "\n",
    "# Create the figure\n",
    "p = figure(plot_width=800, plot_height=800, title=\"Performance histogram\",\n",
    "           toolbar_location=None, tools=\"\")\n",
    "p.xgrid.grid_line_color = None\n",
    "p.xaxis.axis_label = \"perf (GFlop/s)\"\n",
    "p.xaxis.major_label_orientation = 1.2\n",
    "p.yaxis.axis_label = \"# occurrences\"\n",
    "p.quad(source=source, bottom=0, top='hist', left='left', right='right', fill_color='blue')\n",
    "p.add_tools(hover)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top performances\n",
    "\n",
    "#### Observations \n",
    "* well at least there are still quite a few rows even in the top 0.5%\n",
    "* hmm ... I don't really see any \"consensus\" in the top performances for larger kernels. Even in the top 0.5%, there are multiple kernels and multiple \"main parameters\" \n",
    "* it seems as though \"grouping\" and \"minblocks\" are the least influential raw parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Top slices of perf. distribution\n",
    "pars_autotuning_top = {\n",
    "    2: list(), \n",
    "    1: list(), \n",
    "    0.5: list()\n",
    "}\n",
    "#pars_autotuning_top2 = list() # top 2%\n",
    "#pars_autotuning_top1 = list() # top 1%\n",
    "#pars_autotuning_top05 = list() # top 0.5%\n",
    "for i, pa in enumerate(pars_autotuning):\n",
    "    print('\\n\\n---------', i)\n",
    "    max_perf = float(pa[perf].max())\n",
    "    max_perf_idx = pa[perf].idxmax()\n",
    "    max_perf_row = pa.loc[max_perf_idx]\n",
    "    max_perf_cond = max_perf_row[mnk + kernel_pars + perf]\n",
    "    display(max_perf_cond)\n",
    "    for perc in pars_autotuning_top.keys():\n",
    "        lim = max_perf - max_perf*perc/100\n",
    "        blob = pa.loc[pa['perf (Gflop/s)'] >= lim]\n",
    "        print('\\ntop', perc, '%')\n",
    "        display(blob[predictors_raw + perf].describe())\n",
    "        pars_autotuning_top[perc].append(blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair plot \n",
    "\n",
    "#### Raw predictors & performance\n",
    "\n",
    "* perf spreads and diminishes with tile_m, tile_n \n",
    "* aside from that, basically there's nothing to see ... :(  \n",
    "\n",
    "#### Features and performance \n",
    "\n",
    "* well no, it does not look a lot better ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "pars_0 = pars_autotuning[3].replace(to_replace={np.NaN: 0})\n",
    "sns.pairplot(pars_0[predictors_raw + perf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "pars_0 = pars_autotuning[3].replace(to_replace={np.NaN: 0})\n",
    "sns.pairplot(pars_0[predictor_features + perf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box and scatter plots\n",
    "\n",
    "Perhaps plots make more sense once some values are binned? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure \n",
    "from bokeh.io import output_notebook, show\n",
    "output_notebook()\n",
    "\n",
    "for f in predictor_features:\n",
    "    print(f)\n",
    "    p = figure(title=f)\n",
    "    p.circle(x=pars_autotuning[3][f], y=pars_autotuning[3]['perf (Gflop/s)'], size=0.5)\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rudimentary feature selection\n",
    "\n",
    "* I'm guessing that univariate selection won't bring much as there are basically no real 1st order relationships between predictors and performance (cf pair plot) \n",
    "\n",
    "* Recursive feature elimination seems t make a lot more sense... but it has to be used with a model si j'ai bien compris ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `RandomForestClassifier`\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Isolate Data, class labels and column values\n",
    "X = iris.iloc[:,0:4]\n",
    "Y = iris.iloc[:,-1]\n",
    "names = iris.columns.values\n",
    "\n",
    "# Build the model\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# Fit the model\n",
    "rfc.fit(X, Y)\n",
    "\n",
    "# Print the results\n",
    "print(\"Features sorted by their score:\")\n",
    "print(sorted(zip(map(lambda x: round(x, 4), rfc.feature_importances_), names), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance with Extra Trees Classifier\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "# load data\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "# feature extraction\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X, Y)\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LogisticRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-8609ca372ea6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# To do with a model :\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRFE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mrfe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRFE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 2nd arg = num features to retain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LogisticRegression' is not defined"
     ]
    }
   ],
   "source": [
    "# To do with a model :\n",
    "from sklearn.feature_selection import RFE\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, 3) # 2nd arg = num features to retain\n",
    "fit = rfe.fit(X, Y)\n",
    "print(\"Num Features: %d\") % fit.n_features_\n",
    "print(\"Selected Features: %s\") % fit.support_\n",
    "print(\"Feature Ranking: %s\") % fit.ranking_\n",
    "\n",
    "# Create the RFE object and rank each pixel\n",
    "svc = SVC(kernel=\"linear\", C=1)\n",
    "rfe = RFE(estimator=svc, n_features_to_select=1, step=1)\n",
    "rfe.fit(X, y)\n",
    "ranking = rfe.ranking_.reshape(digits.images[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFE with CV\n",
    "# Create the RFE object and compute a cross-validated score.\n",
    "svc = SVC(kernel=\"linear\")\n",
    "# The \"accuracy\" scoring is proportional to the number of correct\n",
    "# classifications\n",
    "rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(2),\n",
    "              scoring='accuracy')\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction with PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "fit = pca.fit(X)\n",
    "# summarize components\n",
    "print(\"Explained Variance: %s\") % fit.explained_variance_ratio_\n",
    "print(fit.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Modeling \n",
    "\n",
    "Modeling for multiple m,n,ks at a time: \n",
    "    Add mnk OR sizes (which?)\n",
    "    Considering adding need_sync\n",
    "\n",
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor \n",
    "X = pars_autotuning[3][predictor_features]\n",
    "n_features = len(list(X.columns))\n",
    "print('Predictor variables:\\n', X.columns)\n",
    "\n",
    "# Outcome\n",
    "Y = pars_autotuning[3][perf]\n",
    "maxperf = float(Y.max(axis=0))\n",
    "print('\\nOutcome variable:\\n', Y.columns, \"with max. value =\", maxperf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from bokeh.plotting import figure \n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "from bokeh.io import output_notebook, show\n",
    "output_notebook()\n",
    "\n",
    "def print_error(y_true, y_pred):\n",
    "    print('Mean Absolute Error:     {:>7.4f}'.format(metrics.mean_absolute_error(y_true, y_pred)))\n",
    "    print('Mean Squared Error:      {:>7.4f}'.format(metrics.mean_squared_error(y_true, y_pred)))\n",
    "    print('Root Mean Squared Error: {:>7.4f}\\n'.format(np.sqrt(metrics.mean_squared_error(y_true, y_pred))))\n",
    "\n",
    "def display_tree(model, model_name):\n",
    "    dot_data = tree.export_graphviz(model, out_file=None, filled=True, \n",
    "                                    leaves_parallel=True, feature_names=list(X.columns))\n",
    "    graph = graphviz.Source(dot_data)\n",
    "    graph.render(model_name)\n",
    "    return graph\n",
    "\n",
    "def perf_pred_hist(y):\n",
    "    # Create histogram and hover tool \n",
    "    num_bins = 200\n",
    "    hist, edges = np.histogram(y, bins=num_bins)\n",
    "    df_hist = pd.DataFrame({'hist': hist, 'left': edges[:-1], 'right': edges[1:]})\n",
    "    source = ColumnDataSource(df_hist)\n",
    "    hover = HoverTool(tooltips=[('# occurences', '@hist'), ('low', '@left'), ('high', '@right')])\n",
    "\n",
    "    # Create the figure\n",
    "    p = figure(plot_width=800, plot_height=800, title=\"Predicted performance histogram\",\n",
    "               toolbar_location=None, tools=\"\")\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.xaxis.axis_label = \"predicted perf (GFlop/s)\"\n",
    "    p.xaxis.major_label_orientation = 1.2\n",
    "    p.yaxis.axis_label = \"# occurrences\"\n",
    "    p.quad(source=source, bottom=0, top='hist', left='left', right='right', fill_color='blue')\n",
    "    p.add_tools(hover)\n",
    "    return p\n",
    "\n",
    "def perf_pred_scatter(y, title, maxperf, maxperf_pred): \n",
    "    p = figure(plot_width=800, plot_height=800, title=title)\n",
    "    max_x = len(y)\n",
    "    x = np.arange(max_x)\n",
    "    p.circle(x=x, y=y, size=4)\n",
    "    p.line(x=[0, max_x], y=[maxperf, maxperf], legend=\"max. perf.\", color='red')\n",
    "    p.line(x=[0, max_x], y=[maxperf_pred, maxperf_pred], legend=\"max. leaf label\", color='green')\n",
    "    p.legend.location = \"bottom_right\"\n",
    "    return p \n",
    "\n",
    "def scatter_top(y, title):\n",
    "    top_m = 50\n",
    "    top_m_idx = np.argpartition(-y, top_m)[:top_m]\n",
    "    y_predmax = Y.iloc[top_m_idx]\n",
    "    p = perf_pred_scatter(y_predmax.values.ravel(), title, maxperf, np.NaN)\n",
    "    show(p)\n",
    "\n",
    "def fit_model(model, x, y): \n",
    "    \n",
    "    # X, Y, split, model\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y.values.ravel(), test_size=0.20, random_state=0)  \n",
    "\n",
    "    # Fit\n",
    "    model.fit(X_train, y_train)\n",
    "    print('\\n-------------------')\n",
    "    print(model, '\\n')\n",
    "    print('Feature importance:')\n",
    "    print(model.feature_importances_, '\\n')\n",
    "\n",
    "    # Training error\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    print('Training error:')\n",
    "    print_error(y_train, y_train_pred)\n",
    "\n",
    "    # Test\n",
    "    y_pred = model.predict(X_test)  \n",
    "    print('Testing error:')\n",
    "    print_error(y_test, y_pred)\n",
    "    \n",
    "    return np.concatenate((y_train_pred, y_pred), axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree & Co \n",
    "\n",
    "#### Observations \n",
    "\n",
    "* DT with friedman mse is basically the same as regular mse, I stopped looking at it \n",
    "* top-classes are too large (size 3'200 and 800 for mse, mae respectively) \n",
    "    * starting from dpeth = 8-9, this gets more reasonable\n",
    "* Maybe DTs are just not discriminatory enough??  hmm ... \n",
    "* IDEAS: get a 2 level tree? (a) identify top class, (b) discriminate further inside of top-class ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import tree\n",
    "import graphviz\n",
    "\n",
    "# Hyperparameters\n",
    "tree_depth = 10\n",
    "min_samples_split = 20 \n",
    "min_samples_leaf = 1\n",
    "top_k = 20\n",
    "\n",
    "def tree_model(model, model_name, X, Y): \n",
    "    \n",
    "    perf = fit_model(model, X, Y)\n",
    "    graph = display_tree(model, model_name)\n",
    "    p = perf_pred_hist(perf)\n",
    "    show(p)\n",
    "\n",
    "    print('highest', top_k, ':')\n",
    "    top = -np.partition(-perf, top_k)[:top_k]\n",
    "    print(top)\n",
    "\n",
    "    top_m = 542\n",
    "    top_m_idx = np.argpartition(-perf, top_m)[:top_m]\n",
    "    y_predmax = Y.iloc[top_m_idx]\n",
    "    \n",
    "    p = perf_pred_scatter(y_predmax.values.ravel(), model_name, maxperf, top[0])\n",
    "    show(p)\n",
    "    return graph\n",
    "\n",
    "# Default DT \n",
    "model_DT = DecisionTreeRegressor(criterion='mse', max_depth=tree_depth)\n",
    "graph_DT = tree_model(model_DT, \"model_DT\", X, Y)\n",
    "\n",
    "# DT with MAE criterion \n",
    "#model_DTa = DecisionTreeRegressor(criterion='mae', max_depth=tree_depth) \n",
    "#graph_DTa = tree_model(model_DTa, \"model_DTa\", X, Y)\n",
    "\n",
    "# DT with Friedman criterion \n",
    "#model_DTf = DecisionTreeRegressor(criterion='friedman_mse', splitter='best', max_depth=tree_depth)\n",
    "#graph_DTf = tree_model(model_DTf, \"model_DTf\", X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "graph_DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph_DTa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest (ensemble method) \n",
    "\n",
    "#### Observations\n",
    "the feature importances are completely off: mostly tile_m and tile_n are used :( \n",
    "I need to write a custom accuracy: \n",
    "    are the k-top among the p% of max perf? och ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model_RF = RandomForestRegressor(bootstrap=False, n_estimators=100, criterion='mse', \n",
    "                                 max_features='auto', max_depth=None, \n",
    "                               min_samples_split=20, min_samples_leaf=1)\n",
    "\n",
    "# Fit\n",
    "perf_RF = fit_model(model_RF, X, Y)\n",
    "\n",
    "# hist \n",
    "p = perf_pred_hist(perf_RF)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter top \n",
    "top_m = 50\n",
    "top_m_idx = np.argpartition(-perf_RF, top_m)[:top_m]\n",
    "y_predmax = Y.iloc[top_m_idx]\n",
    "\n",
    "p = perf_pred_scatter(y_predmax.values.ravel(), \"RF\", maxperf, np.NaN)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree with AdaBoost (ensemble method) \n",
    "\n",
    "#### Observations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "model_AB = AdaBoostRegressor(DecisionTreeRegressor(),\n",
    "                             n_estimators=300, random_state=0)\n",
    "perf_AB = fit_model(model_AB, X, Y)\n",
    "p = perf_pred_hist(perf_AB)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter top \n",
    "top_m = 50\n",
    "top_m_idx = np.argpartition(-perf_AB, top_m)[:top_m]\n",
    "y_predmax = Y.iloc[top_m_idx]\n",
    "\n",
    "p = perf_pred_scatter(y_predmax.values.ravel(), \"AB\", maxperf, np.NaN)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting regressor (ensemble method) \n",
    "\n",
    "#### Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "model_GBR = GradientBoostingRegressor(n_estimators=500, random_state=0, learning_rate=0.1)\n",
    "perf_GBR = fit_model(model_GBR, X, Y)\n",
    "p = perf_pred_hist(perf_GBR)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter top \n",
    "top_m = 50\n",
    "top_m_idx = np.argpartition(-perf_GBR, top_m)[:top_m]\n",
    "y_predmax = Y.iloc[top_m_idx]\n",
    "\n",
    "p = perf_pred_scatter(y_predmax.values.ravel(), \"GBR\", maxperf, np.NaN)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging regressor (over DT) \n",
    "\n",
    "#### Observations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "model_BR = BaggingRegressor()\n",
    "perf_BR = fit_model(model_BR, X, Y)\n",
    "show(perf_pred_hist(perf_BR))\n",
    "show(scatter_top(perf_BR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial regression\n",
    "\n",
    "#### Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR (Support Vector Regression) \n",
    "\n",
    "#### Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "#svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
    "#perf_svr_rbf = fit_model(svr_rbf, X, Y)\n",
    "#p = perf_pred_hist(perf_svr_rbf)\n",
    "#show(p)\n",
    "\n",
    "svr_lin = SVR(kernel='linear', C=1e3)\n",
    "perf_svr_lin = fit_model(svr_lin, X, Y)\n",
    "p = perf_pred_hist(perf_svr_lin)\n",
    "show(p)\n",
    "\n",
    "# scatter top \n",
    "top_m = 50\n",
    "top_m_idx = np.argpartition(-perf_svr_lin, top_m)[:top_m]\n",
    "y_predmax = Y.iloc[top_m_idx]\n",
    "\n",
    "p = perf_pred_scatter(y_predmax.values.ravel(), \"SVR-lin\", maxperf, np.NaN)\n",
    "show(p)\n",
    "\n",
    "#svr_poly = SVR(kernel='poly', C=1e3, degree=2)\n",
    "#perf_svr_poly = fit_model(svr_poly, X, Y)\n",
    "#p = perf_pred_hist(perf_svr_poly)\n",
    "#show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPR (Gaussian Process Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP (Multi Layer Perceptron) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
